{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Khalid-2402/big-data/blob/main/018_MC_Frozenlake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4caa157c",
      "metadata": {
        "id": "4caa157c"
      },
      "outputs": [],
      "source": [
        "# Monte Carlo every-visit using Q table on Frozen Lake"
      ]
    },
    {
      "source": [
        "!pip install gymnasium"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5RuGlZ0NaQv",
        "outputId": "8dbff77c-f883-4fd5-c95b-962c402d1e4a"
      },
      "id": "Z5RuGlZ0NaQv",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!pip install session_info"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIWbsYKwNr0k",
        "outputId": "166aa782-2797-4856-c05a-e83232e185eb"
      },
      "id": "fIWbsYKwNr0k",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting session_info\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting stdlib_list (from session_info)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session_info\n",
            "  Building wheel for session_info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session_info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=2b986061047bd001a36110552b5966b4f3045944fb8f433b47ac558594ccdc13\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session_info\n",
            "Installing collected packages: stdlib_list, session_info\n",
            "Successfully installed session_info-1.0.0 stdlib_list-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0baf0120",
      "metadata": {
        "id": "0baf0120"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import session_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "da12a811",
      "metadata": {
        "id": "da12a811"
      },
      "outputs": [],
      "source": [
        "n_episodes = 500000\n",
        "current_epsilon = 1.0\n",
        "max_epsilon = 1.0\n",
        "min_epsilon = 0.001\n",
        "decay_rate = 0.0001\n",
        "Reward_list = []\n",
        "\n",
        "\n",
        "# Hyperparameters\n",
        "n_episodes = 50000\n",
        "max_steps = 100\n",
        "gamma = 0.99  # Discount factor\n",
        "alpha = 0.1  # Learning rate\n",
        "epsilon_max = 1.0\n",
        "epsilon_min = 0.01\n",
        "epsilon_decay = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e5e2c9d4",
      "metadata": {
        "id": "e5e2c9d4"
      },
      "outputs": [],
      "source": [
        "def print_policy(Q, env, cols, rows):\n",
        "    def action_to_symbol(action):\n",
        "        return ['←', '↓', '→', '↑'][action]\n",
        "\n",
        "    policy = np.zeros((rows, cols), dtype=str)\n",
        "    for state in range(env.observation_space.n):\n",
        "        if np.sum(Q[state]) == 0:\n",
        "            policy[state // cols, state % cols] = 'o'\n",
        "        else:\n",
        "            best_action = np.argmax(Q[state])\n",
        "            policy[state // cols, state % cols] = action_to_symbol(best_action)\n",
        "\n",
        "    # Mark special positions\n",
        "    desc = env.unwrapped.desc\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            if desc[i][j] in b'GH':\n",
        "                policy[i, j] = desc[i][j].decode('utf-8')\n",
        "    policy[0, 0] = 'S'\n",
        "\n",
        "    print(\"=== Learned Policy ===\")\n",
        "    print()\n",
        "    for row in policy:\n",
        "        print(' '.join(row))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9671ebdc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9671ebdc",
        "outputId": "a5af36b1-f7fb-4731-8c3f-43c4f97ae4a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 1000, Average Reward (last 1000 episodes): 0.01\n",
            "Episode: 2000, Average Reward (last 1000 episodes): 0.02\n",
            "Episode: 3000, Average Reward (last 1000 episodes): 0.02\n",
            "Episode: 4000, Average Reward (last 1000 episodes): 0.03\n",
            "Episode: 5000, Average Reward (last 1000 episodes): 0.05\n",
            "Episode: 6000, Average Reward (last 1000 episodes): 0.03\n",
            "Episode: 7000, Average Reward (last 1000 episodes): 0.05\n",
            "Episode: 8000, Average Reward (last 1000 episodes): 0.05\n",
            "Episode: 9000, Average Reward (last 1000 episodes): 0.06\n",
            "Episode: 10000, Average Reward (last 1000 episodes): 0.08\n",
            "Episode: 11000, Average Reward (last 1000 episodes): 0.09\n",
            "Episode: 12000, Average Reward (last 1000 episodes): 0.10\n",
            "Episode: 13000, Average Reward (last 1000 episodes): 0.10\n",
            "Episode: 14000, Average Reward (last 1000 episodes): 0.14\n",
            "Episode: 15000, Average Reward (last 1000 episodes): 0.11\n",
            "Episode: 16000, Average Reward (last 1000 episodes): 0.16\n",
            "Episode: 17000, Average Reward (last 1000 episodes): 0.16\n",
            "Episode: 18000, Average Reward (last 1000 episodes): 0.21\n",
            "Episode: 19000, Average Reward (last 1000 episodes): 0.24\n",
            "Episode: 20000, Average Reward (last 1000 episodes): 0.25\n",
            "Episode: 21000, Average Reward (last 1000 episodes): 0.24\n",
            "Episode: 22000, Average Reward (last 1000 episodes): 0.29\n",
            "Episode: 23000, Average Reward (last 1000 episodes): 0.30\n",
            "Episode: 24000, Average Reward (last 1000 episodes): 0.33\n",
            "Episode: 25000, Average Reward (last 1000 episodes): 0.35\n",
            "Episode: 26000, Average Reward (last 1000 episodes): 0.36\n",
            "Episode: 27000, Average Reward (last 1000 episodes): 0.39\n",
            "Episode: 28000, Average Reward (last 1000 episodes): 0.42\n",
            "Episode: 29000, Average Reward (last 1000 episodes): 0.43\n",
            "Episode: 30000, Average Reward (last 1000 episodes): 0.46\n",
            "Episode: 31000, Average Reward (last 1000 episodes): 0.51\n",
            "Episode: 32000, Average Reward (last 1000 episodes): 0.54\n",
            "Episode: 33000, Average Reward (last 1000 episodes): 0.55\n",
            "Episode: 34000, Average Reward (last 1000 episodes): 0.56\n",
            "Episode: 35000, Average Reward (last 1000 episodes): 0.59\n",
            "Episode: 36000, Average Reward (last 1000 episodes): 0.61\n",
            "Episode: 37000, Average Reward (last 1000 episodes): 0.65\n",
            "Episode: 38000, Average Reward (last 1000 episodes): 0.69\n",
            "Episode: 39000, Average Reward (last 1000 episodes): 0.71\n",
            "Episode: 40000, Average Reward (last 1000 episodes): 0.76\n",
            "Episode: 41000, Average Reward (last 1000 episodes): 0.77\n",
            "Episode: 42000, Average Reward (last 1000 episodes): 0.79\n",
            "Episode: 43000, Average Reward (last 1000 episodes): 0.83\n",
            "Episode: 44000, Average Reward (last 1000 episodes): 0.82\n",
            "Episode: 45000, Average Reward (last 1000 episodes): 0.87\n",
            "Episode: 46000, Average Reward (last 1000 episodes): 0.90\n",
            "Episode: 47000, Average Reward (last 1000 episodes): 0.91\n",
            "Episode: 48000, Average Reward (last 1000 episodes): 0.93\n",
            "Episode: 49000, Average Reward (last 1000 episodes): 0.95\n",
            "Episode: 50000, Average Reward (last 1000 episodes): 0.98\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "# Initialize the environment\n",
        "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False)\n",
        "\n",
        "# Initialize Q-table\n",
        "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "N = np.zeros((env.observation_space.n, env.action_space.n))  # For tracking visit counts\n",
        "\n",
        "def epsilon_greedy_policy(state, epsilon):  # Explotaition vs Exploration\n",
        "    if np.random.random() < epsilon:\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        return np.argmax(Q[state])\n",
        "\n",
        "total_rewards = []\n",
        "\n",
        "for episode in range(n_episodes):\n",
        "    state, _ = env.reset()\n",
        "    epsilon = max(epsilon_min, epsilon_max - (epsilon_max - epsilon_min) * (episode / n_episodes))\n",
        "\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action = epsilon_greedy_policy(state, epsilon)\n",
        "\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    # Calculate returns and update Q-table\n",
        "    G = 0\n",
        "    for t in range(len(episode_states) - 1, -1, -1):\n",
        "        state = episode_states[t]\n",
        "        action = episode_actions[t]\n",
        "        G = gamma * G + episode_rewards[t]\n",
        "\n",
        "        N[state, action] += 1\n",
        "        Q[state, action] += (alpha * (G - Q[state, action]))\n",
        "\n",
        "    total_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    # Print progress\n",
        "    if (episode + 1) % 1000 == 0:\n",
        "        avg_reward = np.mean(total_rewards[-1000:])\n",
        "        print(f\"Episode: {episode + 1}, Average Reward (last 1000 episodes): {avg_reward:.2f}\")\n",
        "\n",
        "print(\"Training completed.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "9f569e89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f569e89",
        "outputId": "e8a4795f-5e0e-463a-8e33-f2c70fd57a6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward over 100 test episodes: 1.00\n"
          ]
        }
      ],
      "source": [
        "# Test the learned policy\n",
        "n_test_episodes = 100\n",
        "test_rewards = []\n",
        "\n",
        "for _ in range(n_test_episodes):\n",
        "    state = env.reset()[0]\n",
        "    episode_reward = 0\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        action = np.argmax(Q[state])\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "    test_rewards.append(episode_reward)\n",
        "\n",
        "print(f\"Average reward over {n_test_episodes} test episodes: {np.mean(test_rewards):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a674ac03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a674ac03",
        "outputId": "167ce4bd-6f1e-42dc-ae75-488304f71015"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Learned Policy ===\n",
            "\n",
            "S ← ↓ ↑\n",
            "↓ H ↓ H\n",
            "→ → ↓ H\n",
            "H → → G\n"
          ]
        }
      ],
      "source": [
        "print_policy(Q,env, 4,4 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0a07e810",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a07e810",
        "outputId": "516a8c35-b9fb-42b2-972e-750eafa741a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----\n",
            "gymnasium           1.0.0\n",
            "numpy               1.26.4\n",
            "session_info        1.0.0\n",
            "-----\n",
            "IPython             7.34.0\n",
            "jupyter_client      6.1.12\n",
            "jupyter_core        5.7.2\n",
            "notebook            6.5.5\n",
            "-----\n",
            "Python 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n",
            "Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "-----\n",
            "Session information updated at 2024-10-31 10:55\n"
          ]
        }
      ],
      "source": [
        "session_info.show(html=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "rl",
      "language": "python",
      "name": "rl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}